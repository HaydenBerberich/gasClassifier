{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'gas_sensor_data.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet is responsible for handling missing values in the dataset. First, it checks for any missing values in each column using the isnull().sum() method, which returns the count of missing values for each column. The results are printed to provide an overview of the missing data. To address the missing values, the code fills them with the mean of their respective columns using the fillna() method. The numeric_only=True parameter ensures that only numeric columns are considered when calculating the mean, thereby preventing errors and ensuring that the dataset is clean and ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Print missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Fill missing values with the mean of the columns\n",
    "data_cleaned = data.fillna(data.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet is responsible for analyzing and handling class imbalance in the dataset. Initially, it calculates the class distribution by counting the occurrences of each class label using the value_counts() method. The results are printed to provide an overview of the class distribution before handling the imbalance. To address the imbalance, the code identifies the majority class and the minority classes. It then resamples the minority classes using the resample() function from sklearn.utils, with replacement, to match the number of instances in the majority class. The resampled minority class data is appended to a list, which is then concatenated to form a balanced dataset. The balanced dataset is shuffled to ensure randomness and reset the index. Finally, the code prints the class distribution after handling the imbalance to verify the changes. This process ensures that the dataset is balanced and ready for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "class_column = 'gas_label'\n",
    "class_distribution = data_cleaned[class_column].value_counts()\n",
    "print(\"Class distribution before handling imbalance:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Handle class imbalance\n",
    "# Separate majority and minority classes\n",
    "majority_class = class_distribution.idxmax()\n",
    "minority_classes = class_distribution[class_distribution != class_distribution.max()].index\n",
    "\n",
    "# Resample minority classes\n",
    "resampled_data = [data_cleaned[data_cleaned[class_column] == majority_class]]\n",
    "for minority_class in minority_classes:\n",
    "    minority_data = data_cleaned[data_cleaned[class_column] == minority_class]\n",
    "    resampled_minority_data = resample(minority_data, \n",
    "                                       replace=True,     # sample with replacement\n",
    "                                       n_samples=class_distribution[majority_class],    # to match majority class\n",
    "                                       random_state=42)  # reproducible results\n",
    "    resampled_data.append(resampled_minority_data)\n",
    "\n",
    "# Combine resampled data\n",
    "balanced_data = pd.concat(resampled_data)\n",
    "\n",
    "# Shuffle the data\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Analyze class distribution after handling imbalance\n",
    "balanced_class_distribution = balanced_data[class_column].value_counts()\n",
    "print(\"Class distribution after handling imbalance:\")\n",
    "print(balanced_class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet is responsible for splitting the dataset into features and the target variable, followed by splitting the data into training and testing sets, and standardizing the features. Initially, the dataset is divided into features (X) and the target variable (y) by dropping the target column from the dataset. The data is then split into training and testing sets using an 80/20 split with the train_test_split function from sklearn.model_selection, ensuring that the class distribution is maintained in both sets by using the stratify parameter. Finally, the features are standardized using the StandardScaler from sklearn.preprocessing. The scaler is fitted on the training data and then applied to both the training and testing data to ensure that the features have a mean of 0 and a standard deviation of 1, which is essential for many machine learning algorithms to perform optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and target variable\n",
    "X = balanced_data.drop(columns=[class_column])\n",
    "y = balanced_data[class_column]\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet is responsible for generating a correlation matrix to identify relationships between features and selecting the most relevant features based on their correlation. Initially, it creates a correlation matrix for the standardized training features using the corr() method from pandas. The correlation matrix is printed to provide an overview of the relationships between the features. To select the most relevant features, the code identifies features with a high correlation (absolute value greater than 0.5) to the target variable. These relevant features are then extracted from both the training and testing datasets, resulting in new datasets (X_train_relevant and X_test_relevant) that contain only the selected features. This process helps in reducing the dimensionality of the data and retaining only the most informative features for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation matrix\n",
    "correlation_matrix = pd.DataFrame(X_train_scaled, columns=X.columns).corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Select features with high correlation to the target variable\n",
    "relevant_features = correlation_matrix.columns[correlation_matrix.abs().max() > 0.5]\n",
    "X_train_relevant = pd.DataFrame(X_train_scaled, columns=X.columns)[relevant_features]\n",
    "X_test_relevant = pd.DataFrame(X_test_scaled, columns=X.columns)[relevant_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet is responsible for training SVM classifiers with both linear and RBF kernels, followed by hyperparameter tuning using GridSearchCV. Initially, it trains an SVM classifier with a linear kernel using the SVC class from sklearn.svm and fits it to the relevant training features (X_train_relevant) and target variable (y_train). Similarly, it trains an SVM classifier with an RBF kernel. To optimize the hyperparameters, the code uses GridSearchCV from sklearn.model_selection. For the linear kernel, it searches over a grid of C values ([0.1, 1, 10, 100]) to find the best regularization parameter. For the RBF kernel, it searches over a grid of C values ([0.1, 1, 10, 100]) and gamma values ([1, 0.1, 0.01, 0.001]) to find the best combination of regularization and kernel parameters. The GridSearchCV performs cross-validation (cv=5) to evaluate the performance of each parameter combination and selects the best model based on the cross-validation results. This process ensures that the SVM classifiers are well-tuned and optimized for the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM classifier with linear kernel\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train_relevant, y_train)\n",
    "\n",
    "# Train SVM classifier with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_rbf.fit(X_train_relevant, y_train)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV for linear kernel\n",
    "param_grid_linear = {'C': [0.1, 1, 10, 100]}\n",
    "grid_search_linear = GridSearchCV(SVC(kernel='linear'), param_grid_linear, cv=5)\n",
    "grid_search_linear.fit(X_train_relevant, y_train)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV for RBF kernel\n",
    "param_grid_rbf = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "grid_search_rbf = GridSearchCV(SVC(kernel='rbf'), param_grid_rbf, cv=5)\n",
    "grid_search_rbf.fit(X_train_relevant, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet is responsible for evaluating the performance of SVM classifiers with both linear and RBF kernels. It first uses the best estimators from the hyperparameter tuning process to make predictions on the test set. The accuracy of each model is then calculated using the accuracy_score function from sklearn.metrics. To provide a detailed evaluation, the code generates confusion matrices for both models using the confusion_matrix function. Additionally, it generates classification reports using the classification_report function, which includes precision, recall, and F1-score for each class. The results are printed to provide a comprehensive evaluation of the models' performance, including accuracy, confusion matrices, and classification reports for both the linear and RBF kernel SVM classifiers. This detailed evaluation helps in understanding the strengths and weaknesses of each model and their effectiveness in classifying the gas types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on the test set\n",
    "y_pred_linear = grid_search_linear.best_estimator_.predict(X_test_relevant)\n",
    "y_pred_rbf = grid_search_rbf.best_estimator_.predict(X_test_relevant)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix_linear = confusion_matrix(y_test, y_pred_linear)\n",
    "conf_matrix_rbf = confusion_matrix(y_test, y_pred_rbf)\n",
    "\n",
    "# Generate classification report\n",
    "class_report_linear = classification_report(y_test, y_pred_linear)\n",
    "class_report_rbf = classification_report(y_test, y_pred_rbf)\n",
    "\n",
    "print(\"Accuracy for SVM with linear kernel:\", accuracy_linear)\n",
    "print(\"Confusion matrix for SVM with linear kernel:\\n\", conf_matrix_linear)\n",
    "print(\"Classification report for SVM with linear kernel:\\n\", class_report_linear)\n",
    "\n",
    "print(\"Accuracy for SVM with RBF kernel:\", accuracy_rbf)\n",
    "print(\"Confusion matrix for SVM with RBF kernel:\\n\", conf_matrix_rbf)\n",
    "print(\"Classification report for SVM with RBF kernel:\\n\", class_report_rbf)\n",
    "\n",
    "print(\"Training and evaluation of SVM classifiers completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
